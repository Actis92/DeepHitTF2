{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, h_dim, dropout_rate=None, activation=tf.nn.relu, kernel_regularizer=None):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.fcn = layers.Dense(h_dim, activation=activation,\n",
    "                                 kernel_initializer='glorot_uniform', kernel_regularizer=kernel_regularizer)\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        \n",
    "    def call(self, x):\n",
    "        for layer in range(self.num_layers):\n",
    "            x = self.fcn(x)\n",
    "            if not self.keep_prob is None:\n",
    "                x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepHit(Model):\n",
    "    def __init__(self, num_layers_shared, h_dim_shared, activation, dropout_rate, kernel_regularizer,\n",
    "                num_layers_cs, h_dim_cs, num_event, num_category):\n",
    "        super(DeepHit, self).__init__()\n",
    "        self.num_event = num_event\n",
    "        self.num_category = num_category\n",
    "        self.shared_net = FCLayer(num_layers_shared, h_dim_shared, dropout_rate, activation, kernel_regularizer)\n",
    "        self.cs_net = FCLayer(num_layers_cs, h_dim_cs, dropout_rate, activation, kernel_regularizer)\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        self.out_net = layers.Dense(num_event * num_category, activation=tf.nn.softmax,\n",
    "                                 kernel_initializer='glorot_uniform', kernel_regularizer=kernel_regularizer)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.shared_net(inputs)\n",
    "        x = layers.Concatenate(axis=1)([inputs, x])\n",
    "        out = []\n",
    "        for _ in range(self.num_event):\n",
    "            cs_out = self.cs_net(x)\n",
    "            out.append(cs_out)\n",
    "        out = tf.stack(out, axis=1) # stack referenced on subject\n",
    "        out = tf.reshape(out, [-1, self.num_event*self.h_dim_cs])\n",
    "        out = self.dropout(out)\n",
    "        out = self.out_net(out)\n",
    "        out = tf.reshape(out, [-1, self.num_event, self.num_category])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this will apply 'softmax' to the logits.\n",
    "def loss_Log_likelihood(x, y, event):\n",
    "    I_1 = tf.math.sign(event)\n",
    "\n",
    "    #for uncenosred: log P(T=t,K=k|x)\n",
    "    tmp1 = tf.math.reduce_sum(tf.math.reduce_sum(x * y, reduction_indices=2), reduction_indices=1, keep_dims=True)\n",
    "    tmp1 = I_1 * log(tmp1)\n",
    "\n",
    "    #for censored: log \\sum P(T>t|x)\n",
    "    tmp2 = tf.math.reduce_sum(tf.math.reduce_sum(x, y, reduction_indices=2), reduction_indices=1, keep_dims=True)\n",
    "    tmp2 = (1. - I_1) * log(tmp2)\n",
    "\n",
    "    return - tf.math.reduce_mean(tmp1 + 1.0*tmp2)\n",
    "\n",
    "# Accuracy metric.\n",
    "def loss_ranking(time, event, num_event, num_category, x, y):\n",
    "    sigma1 = tf.constant(0.1, dtype=tf.float32)\n",
    "    eta = []\n",
    "    for e in range(num_event):\n",
    "        one_vector = tf.ones_like(time, dtype=tf.float32)\n",
    "        I_2 = tf.cast(tf.math.equal(event, e+1), dtype = tf.float32) #indicator for event\n",
    "        I_2 = tf.linalg.diag(tf.squeeze(I_2))\n",
    "        tmp_e = tf.reshape(tf.slice(y, [0, e, 0], [-1, 1, -1]), [-1, num_category]) #event specific joint prob.\n",
    "\n",
    "        R = tf.linalg.matmul(tmp_e, tf.transpose(x)) #no need to divide by each individual dominator\n",
    "        # r_{ij} = risk of i-th pat based on j-th time-condition (last meas. time ~ event time) , i.e. r_i(T_{j})\n",
    "\n",
    "        diag_R = tf.reshape(tf.linalg.diag_part(R), [-1, 1])\n",
    "        R = tf.linalg.matmul(one_vector, tf.transpose(diag_R)) - R # R_{ij} = r_{j}(T_{j}) - r_{i}(T_{j})\n",
    "        R = tf.transpose(R)                                 # Now, R_{ij} (i-th row j-th column) = r_{i}(T_{i}) - r_{j}(T_{i})\n",
    "\n",
    "        T = tf.nn.relu(tf.math.sign(tf.linalg.matmul(one_vector, tf.transpose(time)) - \n",
    "                               tf.linalg.matmul(time, tf.transpose(one_vector))))\n",
    "        # T_{ij}=1 if t_i < t_j  and T_{ij}=0 if t_i >= t_j\n",
    "\n",
    "        T = tf.linalg.matmul(I_2, T) # only remains T_{ij}=1 when event occured for subject i\n",
    "\n",
    "        tmp_eta = tf.math.reduce_mean(T * tf.math.exp(-R/sigma1), reduction_indices=1, keep_dims=True)\n",
    "\n",
    "        eta.append(tmp_eta)\n",
    "    eta = tf.stack(eta, axis=1) #stack referenced on subjects\n",
    "    eta = tf.math.reduce_mean(tf.reshape(eta, [-1, num_event]), reduction_indices=1, keep_dims=True)\n",
    "\n",
    "    return tf.math.reduce_sum(eta)\n",
    "\n",
    "def loss_calibration(time, event, num_event, num_category, x, y):\n",
    "    eta = []\n",
    "    for e in range(num_event):\n",
    "        one_vector = tf.ones_like(time, dtype=tf.float32)\n",
    "        I_2 = tf.cast(tf.math.equal(event, e+1), dtype = tf.float32) #indicator for event\n",
    "        tmp_e = tf.reshape(tf.slice(y, [0, e, 0], [-1, 1, -1]), [-1, num_category]) #event specific joint prob.\n",
    "\n",
    "        r = tf.math.reduce_sum(tmp_e * x, axis=0) #no need to divide by each individual dominator\n",
    "        tmp_eta = tf.math.reduce_mean((r - I_2)**2, reduction_indices=1, keep_dims=True)\n",
    "\n",
    "        eta.append(tmp_eta)\n",
    "    eta = tf.stack(eta, axis=1) #stack referenced on subjects\n",
    "    eta = tf.math.reduce_mean(tf.reshape(eta, [-1, num_event]), reduction_indices=1, keep_dims=True)\n",
    "\n",
    "    return tf.math.reduce_sum(eta) #sum over num_Events\n",
    "\n",
    "\n",
    "# Stochastic gradient descent optimizer.\n",
    "optimizer = tf.optimizers.Adam(learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
